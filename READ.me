# Résumé de Texte avec IA - Projet Django

Exécution du Projet
Pour exécuter ce projet, suivez les étapes ci-dessous :

Après avoir téléchargé le fichier ZIP, placez vous dans la zone dans laquelle vous l'avez enregistré.

Créer et activer un environnement virtuel via l'invite de commande : 
python3 -m venv myenv #installer la dernière version de python si ce n'est pas déjà fait, la version 3.11.2 est suffisante
source myenv/bin/activate  # Sur Windows utilisez `myenv\Scripts\activate`

Installer les dépendances :
pip install -r requirements.txt

Appliquer les migrations :
python manage.py migrate

Démarrer le serveur Django :
python manage.py runserver

Accéder à l'application :
Ouvrez votre navigateur et allez à l'adresse http://127.0.0.1:8000/ pour accéder à l'application.


## Objectif

Le but de ce projet est de développer une application Django permettant de générer des résumés de textes en utilisant trois approches différentes : supervisée, non supervisée, et une approche basée sur un modèle T5 et Sumy pour la comparaison.

## Description des Approches

1. **Résumé supervisé** : Utilisation du modèle BART pour générer des résumés.
2. **Résumé non supervisé** : Utilisation de l'algorithme TextRank et TF-IDF pour extraire les phrases les plus pertinentes.
3. **Résumé T5** : Utilisation du modèle T5 pour générer des résumés.
4. **Résumé Sumy** : Utilisation de la bibliothèque Sumy pour générer des résumés basés sur la méthode LSA (Latent Semantic Analysis).

## Étapes et Algorithmes

### 1. Prétraitement du Texte (Texte Manning)

**Étapes :**
- **Tokenisation** : Séparer le texte en mots ou phrases.
- **Vectorisation** : Convertir les mots ou phrases en vecteurs numériques.

**Algorithme utilisé :**
- `TfidfVectorizer` de la bibliothèque `sklearn` pour transformer le texte en une matrice TF-IDF.

```python
def preprocess_text(document):
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform([document])
    return tfidf_matrix, vectorizer
```

### 2. Machine Learning Supervisé

**Étapes :**
- **Modèle utilisé** : BART (Bidirectional and Auto-Regressive Transformers) de `transformers`.
- **Résumé généré** : Le modèle BART est entraîné pour générer des résumés à partir du texte fourni.

**Algorithme utilisé :**
- `facebook/bart-large-cnn` de la bibliothèque `transformers`.

```python
def generate_summary_supervised(text):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    summary = summarizer(text, max_length=150, min_length=40, do_sample=False)
    return summary[0]['summary_text']
```

**Pourquoi BART ? :**

J'ai choisi BART (Bidirectional and Auto-Regressive Transformers) pour le résumé supervisé en raison de ses caractéristiques et de ses performances spécifiques dans les tâches de génération de texte et de résumé.En particulier, BART est pré-entraîné sur une tâche de débruitage de texte, ce qui le rend très efficace pour comprendre et générer du texte en langage naturel. De plus, il a été fine-tuné spécifiquement pour des tâches de résumé, ce qui lui donne un avantage en termes de performance par rapport à (par exemple) GPT, qui est plus généraliste.


### 3. Machine Learning Non Supervisé

**Étapes :**
- **Tokenisation**: Diviser le texte en phrases.
- **TF-IDF** : Calculer la matrice TF-IDF pour les phrases.
- **Similarité Cosinus** : Calculer la similarité entre les phrases.
- **TextRank** : Utiliser un algorithme de PageRank pour identifier les phrases les plus importantes.

**Algorithme utilisé :**
- `TfidfVectorizer` pour la vectorisation.
- `cosine_similarity` pour la similarité entre phrases.
- `pagerank` de `networkx` pour le score des phrases.

```python
def generate_summary_unsupervised(text, top_n=5):
    sentences = sent_tokenize(text)
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(sentences)
    similarity_matrix = cosine_similarity(tfidf_matrix)
    nx_graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(nx_graph)
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    top_sentences = [ranked_sentences[i][1] for i in range(top_n)]
    summary = ' '.join(top_sentences)
    return summary
``` 
**Pourquoi TF-IDF et TextRank ? :**

J'ai choisi d'utiliser TF-IDF (Term Frequency-Inverse Document Frequency) et TextRank pour le résumé non supervisé en raison de leur efficacité et simplicité dans l'extraction des informations importantes des textes sans nécessiter de données annotées. TF-IDF est une méthode éprouvée et efficace pour convertir le texte en vecteurs numériques en pondérant l'importance des mots dans un document par rapport à un corpus. C'est un algorithme relativement simple à implémenter et à comprendre, ce qui le rend idéal pour de nombreuses applications de traitement du langage naturel. TextRank utilise une variante de l'algorithme PageRank, initialement développé pour classer les pages web, ce qui en fait un outil puissant pour évaluer l'importance des phrases dans un texte. Contrairement aux méthodes supervisées, TextRank ne nécessite pas de corpus de textes annotés pour fonctionner, ce qui en fait une solution idéale lorsque les données annotées sont rares ou inexistantes. La similarité cosinus est une mesure efficace pour évaluer la similarité entre deux vecteurs (phrases), ce qui est crucial pour identifier les phrases les plus représentatives du texte. En utilisant ces techniques, nous pouvons générer des résumés non supervisés qui sont à la fois pertinents et informatifs, tout en étant relativement faciles à mettre en œuvre.


### 4. Résumé avec T5 pour une comparaison avec le machine learning supervisé

**Étapes :**
- **Modèle utilisé** : T5 (Text-To-Text Transfer Transformer).
- **Résumé généré** : Le modèle T5 est entraîné pour générer des résumés à partir du texte fourni.

**Algorithme utilisé :**
- `t5-small` de la bibliothèque `transformers`.

```python
def generate_summary_t5(text):
    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
    t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')
    inputs = t5_tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    outputs = t5_model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### 5. Résumé avec Sumy pour une comparaison avec le machine learning non supervisé

**Étapes :**
- **Tokenisation** : Diviser le texte en phrases.
- **LSA** : Utiliser l'algorithme LSA (Latent Semantic Analysis) pour résumer le texte.

**Algorithme utilisé :**
- `LsaSummarizer` de la bibliothèque `sumy`.

```python
def generate_summary_sumy(text, sentence_count=3):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, sentence_count)
    return " ".join(str(sentence) for sentence in summary)
```

### Conclusion

Chaque méthode de résumé a ses propres avantages et inconvénients. Le résumé supervisé avec BART et T5 permet de générer des résumés plus précis et cohérents, tandis que l'approche non supervisée avec TextRank offre une méthode rapide et efficace sans nécessiter de données d'entraînement. Sumy, avec LSA, fournit une autre approche non supervisée basée sur l'analyse sémantique latente.



















